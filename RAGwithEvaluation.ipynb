{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://learn.next.courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/about). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints._common import NVEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved NVIDIA_API_KEY beginning with \"nvapi-QpK...\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
       " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
       " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
       " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
       " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
       " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
       " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
       " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
       " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
       " 'playground_sdxl_turbo': '0ba5e4c7-4540-4a02-b43a-43980067f4af',\n",
       " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
       " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
       " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
       " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
       " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
       " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
       " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
       " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
       " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
       " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
       " 'playground_sd_video': 'a529a395-a7a0-4708-b4df-eb5e41d5ff60'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        assert not hard_reset\n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/docs/guides/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/docs/guides/evaluation/examples/comparisons). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retreival Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      "Constructed aggregate docstore with 225 chunks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Summary: Large pre-trained language models have been shown to store factual knowledge</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in their parameters, and achieve state-of-the-art results when fine-tuned on</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">downstream NLP tasks. However, their ability to access and precisely manipulate</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge is still limited, and hence on knowledge-intensive tasks, their</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance lags behind task-specific architectures. Additionally, providing</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provenance for their decisions and updating their world knowledge remain open</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">research problems. Pre-trained models with a differentiable access mechanism to</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">explicit non-parametric memory can overcome this issue, but have so far been</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">only investigated for extractive downstream tasks. We explore a general-purpose</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fine-tuning recipe for retrieval-augmented generation (RAG) -- models which</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">combine pre-trained parametric and non-parametric memory for language</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation. We introduce RAG models where the parametric memory is a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-trained seq2seq model and the non-parametric memory is a dense vector index</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">formulations, one which conditions on the same retrieved passages across the</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">whole generated sequence, the other can use different passages per token. We</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fine-tune and evaluate our models on a wide range of knowledge-intensive NLP</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks and set the state-of-the-art on three open domain QA tasks, outperforming</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric seq2seq models and task-specific retrieve-and-extract architectures.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For language generation tasks, we find that RAG models generate more specific,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">diverse and factual language than a state-of-the-art parametric-only seq2seq</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">baseline.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Page Body: RAG-S This 14th century work is divided into </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> sections: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Inferno\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Purgatorio\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> &amp; </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Paradiso\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-way classiﬁcation, we compare against Thorne and Vlachos [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], who train RoBERTa [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">within </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of this model, despite being supplied with only the claim and retrieving its own evidence.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We also analyze whether documents retrieved by RAG correspond to documents annotated as gold</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">71</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of cases, and a gold article is present in the top </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> retrieved articles in </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of cases.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.5</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additional Results</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Generation Diversity</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Section </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> shows that RAG models are more factual and speciﬁc than</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BART for Jeopardy question generation. Following recent work on diversity-promoting decoding</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mPaper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSummary: Large pre-trained language models have been shown to store factual knowledge\u001b[0m\n",
       "\u001b[1;38;2;118;185;0min their parameters, and achieve state-of-the-art results when fine-tuned on\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdownstream NLP tasks. However, their ability to access and precisely manipulate\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge is still limited, and hence on knowledge-intensive tasks, their\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance lags behind task-specific architectures. Additionally, providing\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovenance for their decisions and updating their world knowledge remain open\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresearch problems. Pre-trained models with a differentiable access mechanism to\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexplicit non-parametric memory can overcome this issue, but have so far been\u001b[0m\n",
       "\u001b[1;38;2;118;185;0monly investigated for extractive downstream tasks. We explore a general-purpose\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m -- models which\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcombine pre-trained parametric and non-parametric memory for language\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration. We introduce RAG models where the parametric memory is a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpre-trained seq2seq model and the non-parametric memory is a dense vector index\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mformulations, one which conditions on the same retrieved passages across the\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhole generated sequence, the other can use different passages per token. We\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks and set the state-of-the-art on three open domain QA tasks, outperforming\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mparametric seq2seq models and task-specific retrieve-and-extract architectures.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFor language generation tasks, we find that RAG models generate more specific,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdiverse and factual language than a state-of-the-art parametric-only seq2seq\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbaseline.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mPage Body: RAG-S This 14th century work is divided into \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m sections: \u001b[0m\u001b[32m\"Inferno\"\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m\"Purgatorio\"\u001b[0m\u001b[1;38;2;118;185;0m & \u001b[0m\u001b[32m\"Paradiso\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFor \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m-way classiﬁcation, we compare against Thorne and Vlachos \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m57\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, who train RoBERTa \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithin \u001b[0m\u001b[1;36m2.7\u001b[0m\u001b[1;38;2;118;185;0m% of this model, despite being supplied with only the claim and retrieving its own evidence.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\u001b[0m\n",
       "\u001b[1;38;2;118;185;0min \u001b[0m\u001b[1;36m71\u001b[0m\u001b[1;38;2;118;185;0m% of cases, and a gold article is present in the top \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m retrieved articles in \u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;38;2;118;185;0m% of cases.\u001b[0m\n",
       "\u001b[1;36m4.5\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAdditional Results\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mGeneration Diversity\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mSection \u001b[0m\u001b[1;36m4.3\u001b[0m\u001b[1;38;2;118;185;0m shows that RAG models are more factual and speciﬁc than\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Optional if pulling from the last notebook, the following should work\n",
    "##   Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"\\nSample Chunk:\\n\\n{format_chunk(docs[len(docs)//2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "061121bf-6cbf-4400-b7a7-4c275f208cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_chunks = format_chunk(docs[len(docs)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fea6b726-73b9-434b-8f13-c68f22a8c1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([\"{'Published': '2019-05-24', 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova', 'Summary': 'We introduce a new language representation model called BERT, which stands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation models, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly conditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be fine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, such as question answering\\\\nand language inference, without substantial task-specific architecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains new\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute improvement).'}\", {}, 'Document'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d495919f-4beb-4cde-88eb-c23698b3ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc = docs[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63e72776-c578-4e1a-ad04-ba520ce45e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_doc = docs[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "33b6d894-6391-4365-8961-53546999e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs)\n",
    "docs_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7507bd22-d008-4112-babd-951b92571ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bfd06753-dbf9-4ca5-b70c-c3a455c706ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_chunks = dict({'first_doc' : first_doc,'second_doc':second_doc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d0a20a7-7300-4c85-8858-3594f1e207f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_doc': \"{'Published': '2019-05-24', 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova', 'Summary': 'We introduce a new language representation model called BERT, which stands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation models, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly conditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be fine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, such as question answering\\\\nand language inference, without substantial task-specific architecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains new\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute improvement).'}\",\n",
       " 'second_doc': \"{'Published': '2021-04-12', 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela', 'Summary': 'Large pre-trained language models have been shown to store factual knowledge\\\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\\\nperformance lags behind task-specific architectures. Additionally, providing\\\\nprovenance for their decisions and updating their world knowledge remain open\\\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\\\nexplicit non-parametric memory can overcome this issue, but have so far been\\\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\\\ncombine pre-trained parametric and non-parametric memory for language\\\\ngeneration. We introduce RAG models where the parametric memory is a\\\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\\\nformulations, one which conditions on the same retrieved passages across the\\\\nwhole generated sequence, the other can use different passages per token. We\\\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\\\nFor language generation tasks, we find that RAG models generate more specific,\\\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\\\nbaseline.'}\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "298504ca-eab2-4572-a242-110bcb60b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000, chunk_overlap=100,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    "# )\n",
    "# ## Split the documents and also filter out stubs (overly short chunks)\n",
    "# print(\"Chunking Documents\")\n",
    "# docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "# docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableLambda(...) middle=[RunnableAssign(mapper={\n",
      "  context: RunnableLambda(lambda x: x),\n",
      "  input: RunnableLambda(lambda x: x)\n",
      "})] last={\n",
      "  output: RunnableLambda(lambda x: chat_prompt | llm)\n",
      "}\n",
      "Hello World{'input': 'Tell me something interesting!', 'context': {'input': 'Tell me something interesting!'}}\n",
      "Sure, I'd be happy to share something interesting with you! Did you know that documents can be retrieved and analyzed using natural language processing and machine learning techniques? This technology is called Document Retrieval, and it's used in various fields such as law, healthcare, and research to quickly and accurately find relevant information in large collections of documents.\n",
      "\n",
      "For example, in the legal field, Document Retrieval can be used to search through thousands of legal documents to find cases that are relevant to a specific case being argued in court. This can save lawyers and judges a significant amount of time and resources.\n",
      "\n",
      "In healthcare, Document Retrieval can be used to analyze electronic health records to identify patterns and trends in patient care, which can help healthcare providers make more informed decisions about treatment plans.\n",
      "\n",
      "And in research, Document Retrieval can be used to search through large collections of academic papers to find the most relevant and up-to-date research on a particular topic.\n",
      "\n",
      "Overall, Document Retrieval is a powerful tool that can help us quickly and accurately find the information we need in a world where we are increasingly overwhelmed with data."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Update as necessary and add new components\n",
    "llm = ChatNVIDIA(model='mixtral_8x7b') | StrOutputParser()\n",
    "embedder = NVIDIAEmbeddings(model='nvolveqa_40k')\n",
    "# vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "context_getter = RunnableLambda(lambda x: x)\n",
    "custom_chain =  RPrint(\"Hello World\") | RunnableAssign({'context' : context_getter,'input': (lambda x:x)})\n",
    "\n",
    "## Chain Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "  ## TODO\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter}) \n",
    "\n",
    "## Chain Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = custom_chain | {'output' : (lambda x:chat_prompt | llm  )} \n",
    "print(generator_chain)## TODO\n",
    "generator_chain = generator_chain | output_puller  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their </span>\n",
       "<span style=\"font-weight: bold\">approach to language representation and processing?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their \u001b[0m\n",
       "\u001b[1mapproach to language representation and processing?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The Transformer model, as described in the Paper: Attention Is All You Need, is a sequence transduction </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model that uses self-attention mechanisms to process input sequences, dispensing with recurrence and convolutions </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">entirely. It modifies the self-attention sub-layer in the decoder stack to prevent positions from attending to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">subsequent positions, ensuring that the predictions for a position can only depend on the known outputs at previous</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">positions. The BERT model, on the other hand, as introduced in the Paper: BERT: Pre-training of Deep Bidirectional </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Transformers for Language Understanding, is a language representation model that pre-trains deep bidirectional </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations from unlabeled text by jointly conditioning on both left and right context in all layers. BERT is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">designed to be fine-tuned with just one additional output layer for a wide range of tasks, such as question </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">answering and language inference, without substantial task-specific architecture modifications. The Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model is used for sequence-to-sequence tasks like machine translation, while BERT is used for tasks that require </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">understanding of language context, like question answering and language inference.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The Transformer model, as described in the Paper: Attention Is All You Need, is a sequence transduction \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel that uses self-attention mechanisms to process input sequences, dispensing with recurrence and convolutions \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mentirely. It modifies the self-attention sub-layer in the decoder stack to prevent positions from attending to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msubsequent positions, ensuring that the predictions for a position can only depend on the known outputs at previous\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpositions. The BERT model, on the other hand, as introduced in the Paper: BERT: Pre-training of Deep Bidirectional \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTransformers for Language Understanding, is a language representation model that pre-trains deep bidirectional \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. BERT is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdesigned to be fine-tuned with just one additional output layer for a wide range of tasks, such as question \u001b[0m\n",
       "\u001b[1;38;2;118;185;0manswering and language inference, without substantial task-specific architecture modifications. The Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel is used for sequence-to-sequence tasks like machine translation, while BERT is used for tasks that require \u001b[0m\n",
       "\u001b[1;38;2;118;185;0munderstanding of language context, like question answering and language inference.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How do BERT and Retrieval-Augmented Generation (RAG) models compare in their ability to handle </span>\n",
       "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How do BERT and Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models compare in their ability to handle \u001b[0m\n",
       "\u001b[1mknowledge-intensive NLP tasks?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: BERT is a model that pre-trains deep bidirectional representations from unlabeled text by jointly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conditioning on both left and right context in all layers. It can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">layer to create state-of-the-art models for a wide range of tasks, including question answering and language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inference, without substantial task-specific architecture modifications. However, its ability to access and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">precisely manipulate knowledge is still limited, and providing provenance for its decisions and updating its world </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge remain open research problems. On the other hand, RAG models combine pre-trained parametric and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">non-parametric memory for language generation. The non-parametric memory in RAG models is a dense vector index of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Wikipedia, accessed with a pre-trained neural retriever. RAG models have been shown to outperform parametric </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">seq2seq models and task-specific retrieve-and-extract architectures on a wide range of knowledge-intensive NLP </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. For language generation tasks, RAG models generate more specific, diverse, and factual language than a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art parametric-only seq2seq baseline. RAG models can also go beyond simple extractive QA and answer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">questions with free-form, abstractive text generation.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: BERT is a model that pre-trains deep bidirectional representations from unlabeled text by jointly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconditioning on both left and right context in all layers. It can be fine-tuned with just one additional output \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlayer to create state-of-the-art models for a wide range of tasks, including question answering and language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minference, without substantial task-specific architecture modifications. However, its ability to access and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprecisely manipulate knowledge is still limited, and providing provenance for its decisions and updating its world \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge remain open research problems. On the other hand, RAG models combine pre-trained parametric and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnon-parametric memory for language generation. The non-parametric memory in RAG models is a dense vector index of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mWikipedia, accessed with a pre-trained neural retriever. RAG models have been shown to outperform parametric \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mseq2seq models and task-specific retrieve-and-extract architectures on a wide range of knowledge-intensive NLP \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks. For language generation tasks, RAG models generate more specific, diverse, and factual language than a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art parametric-only seq2seq baseline. RAG models can also go beyond simple extractive QA and answer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestions with free-form, abstractive text generation.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the Mistral 7B model compare to Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> in various tasks, and what techniques does </span>\n",
       "<span style=\"font-weight: bold\">it use for faster inference and handling sequences of arbitrary length?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How does the Mistral 7B model compare to Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m in various tasks, and what techniques does \u001b[0m\n",
       "\u001b[1mit use for faster inference and handling sequences of arbitrary length?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The Mistral 7B model outperforms both Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in various benchmarks, particularly in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasoning, mathematics, and code generation tasks. It achieves this by utilizing grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">faster inference and sliding window attention (SWA) to effectively handle sequences of arbitrary length with a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reduced inference cost. The model also provides a fine-tuned version, Mistral 7B -- Instruct, which surpasses the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B -- Chat model on human and automated benchmarks. Additionally, Mistral 7B has shown a preference of </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4143</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> over Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B in a comparison of outputs from both models. However, Mistral 7B performs similarly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B on knowledge benchmarks, likely due to its limited parameter count.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The Mistral 7B model outperforms both Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in various benchmarks, particularly in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreasoning, mathematics, and code generation tasks. It achieves this by utilizing grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfaster inference and sliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to effectively handle sequences of arbitrary length with a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreduced inference cost. The model also provides a fine-tuned version, Mistral 7B -- Instruct, which surpasses the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B -- Chat model on human and automated benchmarks. Additionally, Mistral 7B has shown a preference of \u001b[0m\n",
       "\u001b[1;36m5020\u001b[0m\u001b[1;38;2;118;185;0m to \u001b[0m\u001b[1;36m4143\u001b[0m\u001b[1;38;2;118;185;0m over Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B in a comparison of outputs from both models. However, Mistral 7B performs similarly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B on knowledge benchmarks, likely due to its limited parameter count.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', '{input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\", synth_questions[-1], \"\", sep='\\n')\n",
    "    pprint(synth_answers[-1], \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "918efa75-5c22-4663-a9e8-28773b02df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usr_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4: [Exercise]** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b36fe2b-5247-4bed-9265-fe9f7048482b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their approach to language representation and processing?',\n",
       " 'Question: How do BERT and Retrieval-Augmented Generation (RAG) models compare in their ability to handle knowledge-intensive NLP tasks?',\n",
       " 'Question: How does the Mistral 7B model compare to Llama 2 and Llama 1 in various tasks, and what techniques does it use for faster inference and handling sequences of arbitrary length?']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World{'input': {'input': 'Question: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their approach to language representation and processing?'}, 'context': {'input': {'input': 'Question: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their approach to language representation and processing?'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their </span>\n",
       "<span style=\"font-weight: bold\">approach to language representation and processing?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How do the Transformer model and the BERT model, both based on attention mechanisms, differ in their \u001b[0m\n",
       "\u001b[1mapproach to language representation and processing?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Transformer model and the BERT model are both built on attention mechanisms, but they differ in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">their approach to language representation and processing.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer model, introduced in the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> by Vaswani et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, is a model </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture for neural machine translation. It uses self-attention mechanisms to process input sequences, allowing</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for more parallelization and reducing the need for recurrence. The Transformer model represents input text as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">continuous vectors and processes them using self-attention layers, enabling it to capture long-range dependencies </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in the data.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, BERT (Bidirectional Encoder Representations from Transformers), presented in the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"BERT: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Pre-training of Deep Bidirectional Transformers for Language Understanding\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> by Devlin et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, is a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-trained language model that uses bidirectional transformers. BERT's primary innovation is the introduction of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">bidirectional self-attention, which processes input text in both directions simultaneously, capturing context from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">both left and right sides of a word. This approach allows BERT to generate more context-aware language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations compared to the Transformer model.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, while both models are based on attention mechanisms, the Transformer model focuses on parallel </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">processing and capturing long-range dependencies, while BERT emphasizes context-aware language representations </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">through bidirectional processing.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sources:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> &amp; Polosukhin, I. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Attention is all you need. In Advances in neural information processing systems (pp. </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5998</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6008</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). BERT: Pre-training of deep bidirectional transformers</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for language understanding. In Proceedings of the </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Conference of the North American Chapter of the Association </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for Computational Linguistics: Human Language Technologies, Volume </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Long and Short Papers) (pp. </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4171</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4186</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Transformer model and the BERT model are both built on attention mechanisms, but they differ in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtheir approach to language representation and processing.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Transformer model, introduced in the paper \u001b[0m\u001b[32m\"Attention is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m by Vaswani et al., \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m, is a model \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitecture for neural machine translation. It uses self-attention mechanisms to process input sequences, allowing\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor more parallelization and reducing the need for recurrence. The Transformer model represents input text as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontinuous vectors and processes them using self-attention layers, enabling it to capture long-range dependencies \u001b[0m\n",
       "\u001b[1;38;2;118;185;0min the data.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, BERT \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mBidirectional Encoder Representations from Transformers\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, presented in the paper \u001b[0m\u001b[32m\"BERT: \u001b[0m\n",
       "\u001b[32mPre-training of Deep Bidirectional Transformers for Language Understanding\"\u001b[0m\u001b[1;38;2;118;185;0m by Devlin et al., \u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m, is a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpre-trained language model that uses bidirectional transformers. BERT's primary innovation is the introduction of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbidirectional self-attention, which processes input text in both directions simultaneously, capturing context from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mboth left and right sides of a word. This approach allows BERT to generate more context-aware language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations compared to the Transformer model.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn summary, while both models are based on attention mechanisms, the Transformer model focuses on parallel \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprocessing and capturing long-range dependencies, while BERT emphasizes context-aware language representations \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthrough bidirectional processing.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSources:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m & Polosukhin, I. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAttention is all you need. In Advances in neural information processing systems \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mpp. \u001b[0m\u001b[1;36m5998\u001b[0m\u001b[1;38;2;118;185;0m-\u001b[0m\u001b[1;36m6008\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. BERT: Pre-training of deep bidirectional transformers\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor language understanding. In Proceedings of the \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m Conference of the North American Chapter of the Association \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor Computational Linguistics: Human Language Technologies, Volume \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLong and Short Papers\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mpp. \u001b[0m\u001b[1;36m4171\u001b[0m\u001b[1;38;2;118;185;0m-\u001b[0m\u001b[1;36m4186\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World{'input': {'input': 'Question: How do BERT and Retrieval-Augmented Generation (RAG) models compare in their ability to handle knowledge-intensive NLP tasks?'}, 'context': {'input': {'input': 'Question: How do BERT and Retrieval-Augmented Generation (RAG) models compare in their ability to handle knowledge-intensive NLP tasks?'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How do BERT and Retrieval-Augmented Generation (RAG) models compare in their ability to handle </span>\n",
       "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How do BERT and Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models compare in their ability to handle \u001b[0m\n",
       "\u001b[1mknowledge-intensive NLP tasks?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the information I retrieved, BERT and Retrieval-Augmented Generation (RAG) models are two </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">different approaches to handling knowledge-intensive NLP tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BERT, which stands for Bidirectional Encoder Representations from Transformers, is a popular transformer-based </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">machine learning technique for NLP tasks. It pre-trains deep bidirectional representations from unlabeled text by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">jointly conditioning on both left and right context in all layers. This allows BERT to create more meaningful </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations of words, taking into account their context within a sentence. BERT has been successful in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">variety of NLP tasks, such as question answering, sentiment analysis, and named entity recognition.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, Retrieval-Augmented Generation (RAG) models are a more recent approach that combines dense </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieval with sequence generation. RAG models first retrieve relevant documents from a large corpus, and then </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generate an answer based on the retrieved documents and the input query. This allows RAG models to handle </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks that require access to a large external knowledge base.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In terms of their ability to handle knowledge-intensive NLP tasks, RAG models have an advantage over BERT because </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">they can access a larger knowledge base beyond the pre-training corpus. However, RAG models require an additional </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">step of document retrieval, which can add to the computational cost and latency of the model.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to summarize, BERT and RAG models are two different approaches to handling knowledge-intensive NLP tasks, with </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BERT relying solely on pre-training and RAG models accessing a larger knowledge base through document retrieval.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sources used:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Devlin, J., Chang, M.W., Lee, K., &amp; Toutanova, K. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). BERT: Pre-training of Deep Bidirectional Transformers </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for Language Understanding. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1810.04805</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Lewis, P., Liu, Y., Goyal, N., &amp; Acharya, U. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Retrieval-Augmented Generation for Knowledge-Intensive NLP </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Tasks. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2005.11401</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the information I retrieved, BERT and Retrieval-Augmented Generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models are two \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdifferent approaches to handling knowledge-intensive NLP tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBERT, which stands for Bidirectional Encoder Representations from Transformers, is a popular transformer-based \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmachine learning technique for NLP tasks. It pre-trains deep bidirectional representations from unlabeled text by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mjointly conditioning on both left and right context in all layers. This allows BERT to create more meaningful \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations of words, taking into account their context within a sentence. BERT has been successful in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvariety of NLP tasks, such as question answering, sentiment analysis, and named entity recognition.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, Retrieval-Augmented Generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models are a more recent approach that combines dense \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mretrieval with sequence generation. RAG models first retrieve relevant documents from a large corpus, and then \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerate an answer based on the retrieved documents and the input query. This allows RAG models to handle \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks that require access to a large external knowledge base.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn terms of their ability to handle knowledge-intensive NLP tasks, RAG models have an advantage over BERT because \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthey can access a larger knowledge base beyond the pre-training corpus. However, RAG models require an additional \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstep of document retrieval, which can add to the computational cost and latency of the model.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSo, to summarize, BERT and RAG models are two different approaches to handling knowledge-intensive NLP tasks, with \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBERT relying solely on pre-training and RAG models accessing a larger knowledge base through document retrieval.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSources used:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m* Devlin, J., Chang, M.W., Lee, K., & Toutanova, K. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. BERT: Pre-training of Deep Bidirectional Transformers \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor Language Understanding. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1810.04805\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Lewis, P., Liu, Y., Goyal, N., & Acharya, U. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Retrieval-Augmented Generation for Knowledge-Intensive NLP \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTasks. arXiv preprint arXiv:\u001b[0m\u001b[1;36m2005.11401\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World{'input': {'input': 'Question: How does the Mistral 7B model compare to Llama 2 and Llama 1 in various tasks, and what techniques does it use for faster inference and handling sequences of arbitrary length?'}, 'context': {'input': {'input': 'Question: How does the Mistral 7B model compare to Llama 2 and Llama 1 in various tasks, and what techniques does it use for faster inference and handling sequences of arbitrary length?'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the Mistral 7B model compare to Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> in various tasks, and what techniques does </span>\n",
       "<span style=\"font-weight: bold\">it use for faster inference and handling sequences of arbitrary length?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How does the Mistral 7B model compare to Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m in various tasks, and what techniques does \u001b[0m\n",
       "\u001b[1mit use for faster inference and handling sequences of arbitrary length?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: I'm here to help answer your questions to the best of my ability using the information provided. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, I don't have access to any documents or databases outside of what has been specifically provided to me in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the context of this conversation.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the information you've given me, I can tell you that there are no specific details about the Mistral 7B </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model, Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, or Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> provided, so I'm unable to compare their performance in various tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for the techniques Mistral 7B uses for faster inference and handling sequences of arbitrary length, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information is also not provided. Generally, models use techniques like dynamic programming, attention mechanisms, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">or caching to handle sequences of arbitrary length more efficiently. For faster inference, models might use methods</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like weight pruning, quantization, or knowledge distillation. But without specific information about the Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model, I can't say for sure what techniques it uses.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I hope this helps, and please let me know if you have any other questions!</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: I'm here to help answer your questions to the best of my ability using the information provided. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mHowever, I don't have access to any documents or databases outside of what has been specifically provided to me in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe context of this conversation.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBased on the information you've given me, I can tell you that there are no specific details about the Mistral 7B \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel, Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m, or Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m provided, so I'm unable to compare their performance in various tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for the techniques Mistral 7B uses for faster inference and handling sequences of arbitrary length, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minformation is also not provided. Generally, models use techniques like dynamic programming, attention mechanisms, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mor caching to handle sequences of arbitrary length more efficiently. For faster inference, models might use methods\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike weight pruning, quantization, or knowledge distillation. But without specific information about the Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel, I can't say for sure what techniques it uses.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mI hope this helps, and please let me know if you have any other questions!\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = \"\"\n",
    "    rag_answer = (rag_chain).invoke({'input': q})\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [],
   "source": [
    "eval_instruction = \"\"\"\n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[0] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[1] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\"\"\"\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', eval_instruction), ('user', '{input}')\n",
    "])\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    usr_msg = f\"Question: {q}\\n\\nAnswer 1: {a_synth}\\n\\n Answer 2: {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'input': usr_msg})]\n",
    "    # pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    # pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [],
   "source": [
    "pref_score = sum((\"[1]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/latest/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/docs/guides/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/docs/modules/agents/concepts) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated less than 30 days ago.**\n",
    "- **Make sure [`35_langserve.ipynb`]() is not occupying port 9012 with a running FastAPI service**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/server_app.py`**](frontend/server_app.py) had several lines of code which trigger the course pass condition. Once the service was deemed healthy, that sequence of code was replaced with `## secret` by another microservice. Your objective is to invoke that series of commands by using your pipeline to pass the Evaluation check! Recall [`35_langserve.ipynb`](35_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NeMo Microservices**](https://developer.nvidia.com/nemo-microservices-early-access), which offers microservice spinup routines that can be deployed on local compute and function similar to AI Playground.\n",
    "- [**NVIDIA AI Workbench**](https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/), which allows for quick and simple model customization workflows that can greatly improve RAG model components for your specific use-cases.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
